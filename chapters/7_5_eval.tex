\chapter{Experimental Evaluation}
\label{chap:experimental-evaluation}

\section{Introduction}
\label{sec:eval-intro}

% TODO: Write brief intro

With comparable I2C WASI interface implementations now available for both WAMR and Wasmtime runtimes, this chapter presents a comprehensive experimental evaluation comparing their \textbf{performance characteristics} and \textbf{resource utilization}. The primary focus centers on a detailed comparative analysis between these two WebAssembly runtime approaches, while leveraging the native implementation as a performance baseline and reference point.

The evaluation encompasses \textbf{timing analysis} across multiple execution phases and comprehensive \textbf{memory profiling} to track allocation patterns. This data is then used for statistical analysis to ensure reliable and reproducible results. By systematically measuring runtime setup overhead, execution latency, and memory consumption, this experimental evaluation provides quantitative insights into the practical trade-offs between different WebAssembly runtime architectures.

This analysis directly addresses the second research question of this thesis: \textbf{RQ2: What is the performance impact of different WebAssembly runtime approaches?} Through controlled experimentation and statistical validation, the evaluation establishes an empirical foundation for understanding when and why specific runtime choices are optimal for embedded I2C applications.

\section{Experimental Setup}
\label{sec:experimental-setup}

\subsection{Hardware Configuration}
\label{subsec:hardware-config}

The experimental testbed consists of a Raspberry Pi acting as the I2C controller, connected to an Arduino Uno serving as the I2C peripheral\footnote{Modern naming conventions for I2C context prefer \textit{controller/peripheral} over \textit{master/slave}}.

\textbf{Raspberry Pi Controller Configuration:}
\begin{itemize}
    \item Model: \textit{[FILL: your Pi model]}
    \item Architecture: aarch64-unknown-linux-musl
    \item Operating System: \textit{[FILL: your OS version]}
    \item I2C Interface: Hardware I2C via GPIO pins \textit{[FILL: specify pins]}
    \item I2C Clock Speed: \textit{[FILL: if you know the frequency]}
\end{itemize}

\textbf{Arduino Uno Peripheral Configuration:}
\begin{itemize}
    \item Model: Arduino Uno (ATmega328P)
    \item I2C Slave Address: \textit{[FILL: your address, e.g., 0x09]}
    \item Firmware: Custom I2C responder implementation
    \item Serial Interface: USB serial for correctness verification (disabled during performance tests)
\end{itemize}

% TODO: Add hardware connection diagram if available
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{figures/hardware-setup}
% \caption{Raspberry Pi and Arduino I2C Communication Setup}
% \label{fig:hardware-setup}
% \end{figure}

\subsection{Arduino Firmware Implementation}
\label{subsec:arduino-firmware}

Two firmware versions were developed for different evaluation phases:

\textbf{Correctness Verification Firmware:}
% TODO: Briefly describe the serial monitor version
The initial firmware version implements echo functionality with serial output for debugging and correctness verification. This version logs all I2C transactions to the serial monitor, allowing verification of successful read/write operations.

\textbf{Performance Testing Firmware:}
% TODO: Describe the optimized version
For performance benchmarks, an optimized firmware version was deployed that eliminates serial communication overhead. Upon receiving any I2C write request, the Arduino responds with a fixed ``hello'' message (\texttt{0x68, 0x65, 0x6c, 0x6c, 0x6f}) for subsequent read operations. This approach minimizes Arduino-side processing variability and ensures consistent response timing.

\subsection{Benchmark Methodology}
\label{subsec:benchmark-methodology}

The evaluation follows a rigorous two-phase methodology to ensure both correctness and measurement reliability.

\subsubsection{Phase 1: Correctness Verification}
\label{subsubsec:correctness-verification}

Prior to performance measurement, functional correctness is verified:
\begin{enumerate}
    \item Deploy correctness verification firmware with serial logging
    \item Execute ping-pong operations across all implementations
    \item Verify successful I2C transactions via Arduino serial monitor output
    \item Confirm data integrity for both write and read operations
\end{enumerate}

% TODO: Consider adding example serial output in appendix
Example verification output demonstrates successful I2C communication (see Appendix~\ref{appendix:serial-logs} for complete logs).

\subsubsection{Phase 2: Performance Measurement}
\label{subsubsec:performance-measurement}

Performance evaluation employs Criterion.rs, a statistical benchmarking framework that provides:
\begin{itemize}
    \item \textbf{Statistical Rigor:} Automatic outlier detection and confidence interval calculation
    \item \textbf{Output Format:} Machine-readable JSON for reproducible analysis
    \item \textbf{Sampling Strategy:} Automatically defined sample size based on measurement stability
\end{itemize}

\textbf{Ping-Pong Operation Definition:}
The benchmarked function performs a complete I2C transaction cycle:
\begin{enumerate}
    \item Open a I2C-resource
    \item Write 5 bytes (\texttt{[0x68, 0x65, 0x6c, 0x6c, 0x6f]}) to I2C slave address
    \item Read 5 bytes from the same I2C slave address
    \item Verify operation completion (no explicit data validation during timing)
\end{enumerate}

\textbf{Measurement Categories:}
\begin{itemize}
    \item \textbf{Runtime Setup:} Time to initialize runtime and prepare for I2C operations
    \item \textbf{Cold Execution:} First ping-pong operation after runtime initialization
    \item \textbf{Hot Execution:} Repeated ping-pong operations in steady state
\end{itemize}

\section{Native Implementation Performance}
\label{sec:native-performance}

% TODO: Present native results separately due to scale differences
As the performance baseline, the native implementation provides reference timing and memory usage characteristics.

\subsection{Native Timing Results}
\label{subsec:native-timing}

Table~\ref{tab:native-performance} presents timing characteristics for the native implementation using linux-embedded-hal.

\begin{table}[htbp]
\centering
\caption{Native Implementation Performance Characteristics}
\label{tab:native-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Measurement Type} & \textbf{Mean (ns)} & \textbf{Median (ns)} & \textbf{Std Dev (ns)} & \textbf{95\% CI} \\
\midrule
% TODO: Fill in your native benchmark results
Runtime Setup    & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Cold Execution   & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Hot Execution    & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

% TODO: Add native performance figure
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.8\textwidth]{figures/native-performance-distribution}
% \caption{Native Implementation Performance Distribution}
% \label{fig:native-performance}
% \end{figure}

\subsection{Native Memory Usage}
\label{subsec:native-memory}

% TODO: Present DHAT results for native implementation
Memory profiling reveals minimal allocation overhead for the native implementation, consistent with direct hardware access patterns.

\section{WebAssembly Implementation Performance}
\label{sec:wasm-performance}

This section presents detailed performance analysis comparing WAMR and Wasmtime implementations.

\subsection{Runtime Setup Performance}
\label{subsec:setup-performance}

Table~\ref{tab:setup-performance} compares runtime initialization overhead between WebAssembly implementations.

\begin{table}[htbp]
\centering
\caption{WebAssembly Runtime Setup Performance}
\label{tab:setup-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Implementation} & \textbf{Mean (ns)} & \textbf{Median (ns)} & \textbf{Std Dev (ns)} & \textbf{95\% CI} \\
\midrule
% TODO: Fill in your WAMR and Wasmtime setup results
WAMR          & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Wasmtime      & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

% TODO: Add setup performance comparison figure
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.9\textwidth]{figures/setup-performance-comparison}
% \caption{Runtime Setup Performance: WAMR vs Wasmtime}
% \label{fig:setup-comparison}
% \end{figure}

\subsection{Execution Performance Analysis}
\label{subsec:execution-performance}

\subsubsection{Cold Execution Comparison}
\label{subsubsec:cold-execution}

Table~\ref{tab:cold-performance} presents first-execution performance after runtime initialization.

\begin{table}[htbp]
\centering
\caption{Cold Execution Performance: WAMR vs Wasmtime}
\label{tab:cold-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Implementation} & \textbf{Mean (ns)} & \textbf{Median (ns)} & \textbf{MAD (ns)} & \textbf{95\% CI} \\
\midrule
% TODO: Fill in cold execution data
WAMR          & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Wasmtime      & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Hot Execution Comparison}
\label{subsubsec:hot-execution}

Table~\ref{tab:hot-performance} shows steady-state performance characteristics.

\begin{table}[htbp]
\centering
\caption{Hot Execution Performance: WAMR vs Wasmtime}
\label{tab:hot-performance}
\begin{tabular}{lrrrr}
\toprule
\textbf{Implementation} & \textbf{Mean (ns)} & \textbf{Median (ns)} & \textbf{MAD (ns)} & \textbf{IQR (ns)} \\
\midrule
% TODO: Fill in hot execution data
WAMR          & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Wasmtime      & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

% TODO: Add execution performance comparison figure
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.9\textwidth]{figures/execution-performance-comparison}
% \caption{Execution Performance Distribution: WAMR vs Wasmtime}
% \label{fig:execution-comparison}
% \end{figure}

\section{Statistical Analysis}
\label{sec:statistical-analysis}

\subsection{Distribution Characteristics}
\label{subsec:distribution-analysis}

% TODO: Analyze the shape and characteristics of your timing distributions
Performance measurements exhibit distinct distribution characteristics between implementations. 

% TODO: Add distribution comparison figure (box plots/violin plots)
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=1.0\textwidth]{figures/performance-distributions}
% \caption{Performance Distribution Comparison: Box Plots}
% \label{fig:performance-distributions}
% \end{figure}

\textbf{Distribution Analysis:}
% TODO: Describe what the distributions look like
\begin{itemize}
    \item \textbf{WAMR:} \textit{[FILL: describe distribution shape, skewness, outliers]}
    \item \textbf{Wasmtime:} \textit{[FILL: describe distribution shape, skewness, outliers]}
    \item \textbf{Native:} \textit{[FILL: describe distribution shape, baseline variability]}
\end{itemize}

\subsection{Significance Testing}
\label{subsec:significance-testing}

Statistical hypothesis testing evaluates whether observed performance differences are statistically significant.

\textbf{Test Methodology:}
% TODO: Describe your statistical testing approach
\begin{itemize}
    \item \textbf{Test Type:} Mann-Whitney U test (non-parametric, suitable for timing data)
    \item \textbf{Null Hypothesis:} No significant difference between WAMR and Wasmtime performance
    \item \textbf{Significance Level:} $\alpha = 0.05$
    \item \textbf{Effect Size:} Cohen's d for practical significance assessment
\end{itemize}

\textbf{Results:}
% TODO: Report your statistical test results
\begin{itemize}
    \item \textbf{Setup Performance:} $p < \textit{[FILL]}$, $d = \textit{[FILL]}$ (large effect)
    \item \textbf{Cold Execution:} $p = \textit{[FILL]}$, $d = \textit{[FILL]}$ (\textit{[FILL]} effect)
    \item \textbf{Hot Execution:} $p = \textit{[FILL]}$, $d = \textit{[FILL]}$ (\textit{[FILL]} effect)
\end{itemize}

\subsection{Performance Variability Analysis}
\label{subsec:variability-analysis}

Coefficient of variation (CV) quantifies relative variability across implementations:

% TODO: Calculate and report CV values
\begin{equation}
CV = \frac{\sigma}{\mu} \times 100\%
\end{equation}

Table~\ref{tab:variability-analysis} presents variability metrics.

\begin{table}[htbp]
\centering
\caption{Performance Variability Comparison}
\label{tab:variability-analysis}
\begin{tabular}{lrrr}
\toprule
\textbf{Implementation} & \textbf{Setup CV (\%)} & \textbf{Cold Exec CV (\%)} & \textbf{Hot Exec CV (\%)} \\
\midrule
% TODO: Calculate CV for each implementation
Native       & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
WAMR         & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Wasmtime     & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

\section{Memory Usage Analysis}
\label{sec:memory-analysis}

DHAT profiling provides detailed memory allocation insights across implementation phases.

\subsection{Runtime Setup Memory Profiling}
\label{subsec:memory-setup}

Table~\ref{tab:memory-setup} presents memory allocation characteristics during runtime initialization.

\begin{table}[htbp]
\centering
\caption{Memory Usage During Runtime Setup}
\label{tab:memory-setup}
\begin{tabular}{lrrrr}
\toprule
\textbf{Implementation} & \textbf{Peak Heap (KB)} & \textbf{Total Allocs} & \textbf{Avg Alloc (B)} & \textbf{Max Alloc (B)} \\
\midrule
% TODO: Fill in DHAT setup results
Native        & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
WAMR          & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Wasmtime      & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Execution Memory Profiling}
\label{subsec:memory-execution}

Table~\ref{tab:memory-execution} shows memory allocation patterns during ping-pong execution.

\begin{table}[htbp]
\centering
\caption{Memory Usage During Ping-Pong Execution}
\label{tab:memory-execution}
\begin{tabular}{lrrrr}
\toprule
\textbf{Implementation} & \textbf{Heap Delta (B)} & \textbf{Exec Allocs} & \textbf{Avg Lifetime (ms)} & \textbf{Fragmentation} \\
\midrule
% TODO: Fill in DHAT execution results
Native        & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
WAMR          & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
Wasmtime      & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Flame Graph Insights}
\label{subsec:flamegraph-analysis}

CPU profiling via flame graph analysis reveals time distribution patterns across function calls.

% TODO: Reference flame graph figure
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=1.0\textwidth]{figures/combined-flamegraph}
% \caption{Combined Flame Graph: CPU Time Distribution}
% \label{fig:flamegraph}
% \end{figure}

% TODO: Brief analysis of flame graph findings
Key observations from flame graph analysis:
\begin{itemize}
    \item \textbf{Hotspot Identification:} \textit{[FILL: major CPU consumers]}
    \item \textbf{Call Stack Depth:} \textit{[FILL: complexity comparison]}
    \item \textbf{I/O vs Computation:} \textit{[FILL: time distribution]}
\end{itemize}

\section{WAMR vs Wasmtime: Detailed Comparison}
\label{sec:detailed-comparison}

This section provides comprehensive analysis of the two WebAssembly runtime approaches.

\subsection{Performance Profile Comparison}
\label{subsec:performance-profiles}

% TODO: Create comprehensive performance comparison
Figure~\ref{fig:performance-radar} presents a multi-dimensional performance comparison.

% TODO: Add radar/spider chart comparing multiple performance aspects
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.8\textwidth]{figures/performance-radar-chart}
% \caption{Multi-dimensional Performance Comparison: WAMR vs Wasmtime}
% \label{fig:performance-radar}
% \end{figure}

\textbf{Performance Characteristics Summary:}
\begin{itemize}
    \item \textbf{Startup Latency:} \textit{[FILL: WAMR advantage analysis]}
    \item \textbf{Steady-State Throughput:} \textit{[FILL: execution performance comparison]}
    \item \textbf{Memory Efficiency:} \textit{[FILL: resource usage comparison]}
    \item \textbf{Performance Predictability:} \textit{[FILL: consistency analysis]}
\end{itemize}

\subsection{Architectural Impact Analysis}
\label{subsec:architectural-impact}

\textbf{WASI Preview Differences:}
% TODO: Analyze how Preview 1 vs Preview 2 affects performance
\begin{itemize}
    \item \textbf{Interface Complexity:} \textit{[FILL: binding overhead analysis]}
    \item \textbf{Type System Impact:} \textit{[FILL: WIT vs handwritten binding performance]}
    \item \textbf{Component Model Overhead:} \textit{[FILL: Wasmtime-specific costs]}
\end{itemize}

\textbf{Runtime Architecture Differences:}
% TODO: Discuss implementation differences affecting performance
\begin{itemize}
    \item \textbf{JIT vs Interpretation:} \textit{[FILL: compilation strategy impact]}
    \item \textbf{Memory Management:} \textit{[FILL: allocation strategy differences]}
    \item \textbf{Host Call Mechanisms:} \textit{[FILL: function call overhead]}
\end{itemize}

\subsection{Embedded Systems Suitability}
\label{subsec:embedded-suitability}

\textbf{Resource Constraint Analysis:}
% TODO: Evaluate suitability for embedded applications
\begin{itemize}
    \item \textbf{Memory Footprint:} \textit{[FILL: total memory requirements]}
    \item \textbf{Startup Time Requirements:} \textit{[FILL: real-time application impact]}
    \item \textbf{Deterministic Behavior:} \textit{[FILL: timing predictability]}
    \item \textbf{Power Consumption Implications:} \textit{[FILL: efficiency considerations]}
\end{itemize}

\textbf{Development Experience Comparison:}
\begin{itemize}
    \item \textbf{Toolchain Maturity:} \textit{[FILL: debugging and profiling support]}
    \item \textbf{Standards Compliance:} \textit{[FILL: WASI specification adherence]}
    \item \textbf{Ecosystem Integration:} \textit{[FILL: library and framework support]}
\end{itemize}

\section{Discussion and Implications}
\label{sec:discussion-implications}

\subsection{Key Findings Synthesis}
\label{subsec:key-findings}

% TODO: Synthesize major findings
The experimental evaluation reveals several critical insights:

\begin{enumerate}
    \item \textbf{Setup Performance Divergence:} \textit{[FILL: analysis of extreme Wasmtime setup overhead]}
    \item \textbf{Execution Performance Convergence:} \textit{[FILL: similar steady-state performance]}
    \item \textbf{Memory Usage Trade-offs:} \textit{[FILL: memory vs performance considerations]}
    \item \textbf{Variability Characteristics:} \textit{[FILL: consistency implications]}
\end{enumerate}

\subsection{Practical Application Guidelines}
\label{subsec:application-guidelines}

Based on experimental findings, implementation selection guidelines emerge:

\textbf{Choose WAMR when:}
% TODO: Define WAMR use cases
\begin{itemize}
    \item \textit{[FILL: scenarios where WAMR excels]}
    \item Frequent runtime instantiation required
    \item Memory-constrained embedded environments
    \item Real-time performance requirements
\end{itemize}

\textbf{Choose Wasmtime when:}
% TODO: Define Wasmtime use cases
\begin{itemize}
    \item \textit{[FILL: scenarios where Wasmtime excels]}
    \item Long-running applications (setup cost amortization)
    \item Standards compliance critical
    \item Development velocity prioritized
\end{itemize}

\textbf{Consider Native when:}
% TODO: Define native use cases
\begin{itemize}
    \item Maximum performance required
    \item Platform lock-in acceptable
    \item Security isolation unnecessary
\end{itemize}

\subsection{Performance Optimization Opportunities}
\label{subsec:optimization-opportunities}

% TODO: Identify specific optimization possibilities
Experimental results suggest several optimization directions:

\begin{itemize}
    \item \textbf{Wasmtime Startup Optimization:} \textit{[FILL: potential improvements]}
    \item \textbf{Batch Operation Strategies:} \textit{[FILL: amortizing overhead]}
    \item \textbf{Memory Pool Techniques:} \textit{[FILL: allocation optimization]}
    \item \textbf{Async I/O Integration:} \textit{[FILL: concurrency benefits]}
\end{itemize}

\subsection{Limitations and Validity Threats}
\label{subsec:limitations}

\textbf{Experimental Limitations:}
% TODO: Acknowledge study limitations
\begin{itemize}
    \item \textbf{Workload Scope:} Limited to simple ping-pong I2C operations
    \item \textbf{Hardware Specificity:} Results specific to Raspberry Pi + Arduino setup
    \item \textbf{Scale Considerations:} Single-operation focus may not represent bulk scenarios
    \item \textbf{Environmental Factors:} Controlled laboratory conditions
\end{itemize}

\textbf{Generalizability Considerations:}
\begin{itemize}
    \item \textbf{Architecture Dependence:} ARM64-specific results
    \item \textbf{I2C Protocol Specificity:} May not generalize to other WASI interfaces
    \item \textbf{Implementation Version Sensitivity:} Results tied to specific runtime versions
\end{itemize}

\section{Conclusion}
\label{sec:eval-conclusion}

% TODO: Concise conclusion summarizing key findings
This experimental evaluation provides comprehensive performance characterization of WebAssembly-based I2C implementations. The analysis reveals \textit{[FILL: primary conclusion about WAMR vs Wasmtime trade-offs]} while establishing \textit{[FILL: practical guidelines for implementation selection]}.

Key contributions include:
\begin{itemize}
    \item Quantitative performance comparison across multiple dimensions
    \item Statistical validation of observed performance differences  
    \item Memory usage characterization for embedded applications
    \item Practical guidelines for runtime selection in I2C applications
\end{itemize}

The findings establish a foundation for informed decision-making in embedded WebAssembly I2C applications and identify specific optimization opportunities for future development.

% Professional touches for reproducibility
\section*{Reproducibility Statement}
\label{sec:reproducibility}

All experimental code, benchmark scripts, and raw data are available in the project repository: \texttt{merlijn-sebrechts/wamr-wasi-i2c}. Benchmarks can be reproduced using the provided \texttt{justfile} automation scripts. DHAT profiling and flame graph generation scripts are included in the \texttt{benchall} directory.

% TODO: Add version information table
\begin{table}[htbp]
\centering
\caption{Software Version Information}
\label{tab:software-versions}
\begin{tabular}{lr}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
% TODO: Fill in exact versions you used
Rust Toolchain     & \textit{[FILL]} \\
WAMR               & \textit{[FILL]} \\
Wasmtime           & \textit{[FILL]} \\
Criterion.rs       & \textit{[FILL]} \\
DHAT               & \textit{[FILL]} \\
\bottomrule
\end{tabular}
\end{table}